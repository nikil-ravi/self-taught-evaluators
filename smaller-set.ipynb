{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from together import Together\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datasets import load_from_disk, Dataset\n",
    "from prompts import MODIFIED_INSTRUCTION_GENERATION_PROMPT, JUDGEMENT_ANNOTATION_PROMPT\n",
    "import logging\n",
    "import json\n",
    "from groq import Groq\n",
    "from typing import Any, Dict\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from self_taught_evaluator import (\n",
    "    load_data,\n",
    "    create_eval_set,\n",
    "    evaluate_model_as_judge,\n",
    "    generate_response,\n",
    "    generate_bad_response,\n",
    "    generate_judgements,\n",
    "    annotate_data,\n",
    "    launch_finetuning_job,\n",
    ")\n",
    "\n",
    "# force reimport of launch_finetuning_job\n",
    "from self_taught_evaluator import launch_finetuning_job\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "NUM_JUDGEMENTS = 5\n",
    "NUM_ITERATIONS = 4\n",
    "INFERENCE_MODEL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keys.json\", \"r\") as f:\n",
    "    import json\n",
    "    keys = json.load(f)\n",
    "    \n",
    "    os.environ[\"TOGETHER_API_KEY\"] = keys[\"TOGETHER_API_KEY\"]\n",
    "\n",
    "api_key = os.environ[\"TOGETHER_API_KEY\"]\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set all required environment variables: TOGETHER_API_KEY\")\n",
    "\n",
    "# Initialize clients\n",
    "together_client = Together(api_key=api_key)\n",
    "\n",
    "# Load dataset\n",
    "data_path = \"processed_data/\"\n",
    "full_dataset = load_from_disk(data_path)\n",
    "\n",
    "# Create directories\n",
    "output_dir = Path(\"finetuning_data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset loaded with {len(full_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "base_model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "suffix = \"self_taught_eval_ft_v3_sanity\"\n",
    "n_epochs = 3\n",
    "n_checkpoints = 1\n",
    "lora = True\n",
    "\n",
    "# Data configuration\n",
    "num_samples = 200\n",
    "eval_set_size = 50\n",
    "\n",
    "indices = list(range(len(full_dataset)))\n",
    "train_indices, eval_indices = train_test_split(\n",
    "    indices, \n",
    "    test_size=min(eval_set_size, int(len(full_dataset) * 0.2)),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_indices = train_indices[:min(num_samples, len(train_indices))]\n",
    "dataset = full_dataset.select(train_indices)\n",
    "eval_dataset = full_dataset.select(eval_indices)\n",
    "\n",
    "print(f\"Training on {len(dataset)} samples\")\n",
    "print(f\"Evaluating on {len(eval_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 1\n",
    "current_model = base_model\n",
    "output_filename = output_dir / f\"annotations_iter{iteration}.jsonl\"\n",
    "\n",
    "model_for_annotation = INFERENCE_MODEL\n",
    "current_client = together_client\n",
    "\n",
    "print(f\"Starting iteration {iteration} - Annotation\")\n",
    "print(f\"Using model: {model_for_annotation}\")\n",
    "\n",
    "# Annotate data\n",
    "annotated_data = annotate_data(dataset, model_for_annotation, current_client, iteration, output_filename)\n",
    "print(f\"Annotated {len(annotated_data)} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_iter = f\"{suffix}_iter{iteration}\"\n",
    "print(f\"Starting iteration {iteration} - Fine-tuning\")\n",
    "print(f\"Suffix: {suffix_iter}\")\n",
    "\n",
    "# Launch fine-tuning\n",
    "ft_resp = launch_finetuning_job(\n",
    "    output_filename,\n",
    "    current_model,\n",
    "    suffix_iter,\n",
    "    n_epochs,\n",
    "    n_checkpoints,\n",
    "    lora,\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuning job launched: {ft_resp.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_to_monitor_id = ft_resp.id\n",
    "new_fine_tuned_model_id = None\n",
    "\n",
    "print(\"Monitoring fine-tuning job...\")\n",
    "while True:\n",
    "    try:\n",
    "        status_response = together_client.fine_tuning.retrieve(job_to_monitor_id)\n",
    "        print(f\"Job {job_to_monitor_id}: Status = {status_response.status}\")\n",
    "        \n",
    "        if status_response.status == 'completed':\n",
    "            new_fine_tuned_model_id = ft_resp.output_name\n",
    "            print(f\"Fine-tuning completed! New model: {new_fine_tuned_model_id}\")\n",
    "            break\n",
    "        elif status_response.status in ['error', 'failed', 'cancelled']:\n",
    "            print(f\"Fine-tuning failed with status: {status_response.status}\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking status: {e}\")\n",
    "    \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_fine_tuned_model_id:\n",
    "    current_model = new_fine_tuned_model_id\n",
    "    print(f\"Updated current model to: {current_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = \"nikilrav/Mixtral-8x7B-Instruct-v0.1-self_taught_eval_ft_v3_sanity_iter1-223a1d08-a5fdb2b8\" # dedicated endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating model...\")\n",
    "first_iter_model_results = evaluate_model_as_judge(\n",
    "    current_model,\n",
    "    together_client,\n",
    "    eval_dataset,\n",
    "    output_dir / f\"{suffix}_iter1_model_eval.jsonl\"\n",
    ")\n",
    "print(f\"First iteration model results: {first_iter_model_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 2\n",
    "output_filename = output_dir / f\"annotations_iter{iteration}.jsonl\"\n",
    "\n",
    "# For iteration 2+, use Together with fine-tuned model\n",
    "model_for_annotation = current_model\n",
    "current_client = together_client\n",
    "\n",
    "print(f\"Starting iteration {iteration} - Annotation\")\n",
    "print(f\"Using model: {model_for_annotation}\")\n",
    "\n",
    "# Annotate data\n",
    "annotated_data = annotate_data(dataset, model_for_annotation, current_client, iteration, output_filename)\n",
    "print(f\"Annotated {len(annotated_data)} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_iter = f\"{suffix}_iter{iteration}\"\n",
    "print(f\"Starting iteration {iteration} - Fine-tuning\")\n",
    "print(f\"Suffix: {suffix_iter}\")\n",
    "\n",
    "# Launch fine-tuning\n",
    "ft_resp = launch_finetuning_job(\n",
    "    output_filename,\n",
    "    base_model,\n",
    "    suffix_iter,\n",
    "    n_epochs,\n",
    "    n_checkpoints,\n",
    "    lora,\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuning job launched: {ft_resp.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_to_monitor_id = ft_resp.id\n",
    "new_fine_tuned_model_id = None\n",
    "\n",
    "print(\"Monitoring fine-tuning job...\")\n",
    "while True:\n",
    "    try:\n",
    "        status_response = together_client.fine_tuning.retrieve(job_to_monitor_id)\n",
    "        print(f\"Job {job_to_monitor_id}: Status = {status_response.status}\")\n",
    "        \n",
    "        if status_response.status == 'completed':\n",
    "            new_fine_tuned_model_id = ft_resp.output_name\n",
    "            print(f\"Fine-tuning completed! New model: {new_fine_tuned_model_id}\")\n",
    "            break\n",
    "        elif status_response.status in ['error', 'failed', 'cancelled']:\n",
    "            print(f\"Fine-tuning failed with status: {status_response.status}\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking status: {e}\")\n",
    "    \n",
    "    time.sleep(30)\n",
    "\n",
    "# Update current model\n",
    "if new_fine_tuned_model_id:\n",
    "    current_model = new_fine_tuned_model_id\n",
    "    print(f\"Updated current model to: {current_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = \"nikilrav/Mixtral-8x7B-Instruct-v0.1-self_taught_eval_ft_v3_sanity_iter2-978d1376-ae723cfc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating model...\")\n",
    "second_iter_model_results = evaluate_model_as_judge(\n",
    "    current_model,\n",
    "    together_client,\n",
    "    eval_dataset,\n",
    "    output_dir / f\"{suffix}_iter2_model_eval.jsonl\"\n",
    ")\n",
    "print(f\"Second iteration model results: {second_iter_model_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 3\n",
    "output_filename = output_dir / f\"annotations_iter{iteration}.jsonl\"\n",
    "\n",
    "# For iteration 3, use Together with fine-tuned model\n",
    "model_for_annotation = current_model\n",
    "current_client = together_client\n",
    "\n",
    "print(f\"Starting iteration {iteration} - Annotation\")\n",
    "print(f\"Using model: {model_for_annotation}\")\n",
    "\n",
    "# Annotate data\n",
    "annotated_data = annotate_data(dataset, model_for_annotation, current_client, iteration, output_filename)\n",
    "print(f\"Annotated {len(annotated_data)} data points\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_iter = f\"{suffix}_iter{iteration}\"\n",
    "print(f\"Starting iteration {iteration} - Fine-tuning\")\n",
    "print(f\"Suffix: {suffix_iter}\")\n",
    "\n",
    "# Launch fine-tuning\n",
    "ft_resp = launch_finetuning_job(\n",
    "    output_filename,\n",
    "    base_model,\n",
    "    suffix_iter,\n",
    "    n_epochs,\n",
    "    n_checkpoints,\n",
    "    lora,\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuning job launched: {ft_resp.id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_to_monitor_id = ft_resp.id\n",
    "new_fine_tuned_model_id = None\n",
    "\n",
    "print(\"Monitoring fine-tuning job...\")\n",
    "while True:\n",
    "    try:\n",
    "        status_response = together_client.fine_tuning.retrieve(job_to_monitor_id)\n",
    "        print(f\"Job {job_to_monitor_id}: Status = {status_response.status}\")\n",
    "        \n",
    "        if status_response.status == 'completed':\n",
    "            new_fine_tuned_model_id = ft_resp.output_name\n",
    "            print(f\"Fine-tuning completed! New model: {new_fine_tuned_model_id}\")\n",
    "            break\n",
    "        elif status_response.status in ['error', 'failed', 'cancelled']:\n",
    "            print(f\"Fine-tuning failed with status: {status_response.status}\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking status: {e}\")\n",
    "    \n",
    "    time.sleep(30)\n",
    "\n",
    "# Update current model\n",
    "if new_fine_tuned_model_id:\n",
    "    current_model = new_fine_tuned_model_id\n",
    "    print(f\"Updated current model to: {current_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = \"nikilrav/Mixtral-8x7B-Instruct-v0.1-self_taught_eval_ft_v3_sanity_iter3-c7c58d52-e448820e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating third iteration model...\")\n",
    "third_iter_model_results = evaluate_model_as_judge(\n",
    "    current_model,\n",
    "    together_client,\n",
    "    eval_dataset,\n",
    "    output_dir / f\"{suffix}_iter3_model_eval.jsonl\"\n",
    ")\n",
    "print(f\"Third iteration model results: {third_iter_model_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 4\n",
    "output_filename = output_dir / f\"annotations_iter{iteration}.jsonl\"\n",
    "\n",
    "# For iteration 3, use Together with fine-tuned model\n",
    "model_for_annotation = current_model\n",
    "current_client = together_client\n",
    "\n",
    "print(f\"Starting iteration {iteration} - Annotation\")\n",
    "print(f\"Using model: {model_for_annotation}\")\n",
    "\n",
    "# Annotate data\n",
    "annotated_data = annotate_data(dataset, model_for_annotation, current_client, iteration, output_filename)\n",
    "print(f\"Annotated {len(annotated_data)} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_iter = f\"{suffix}_iter{iteration}\"\n",
    "print(f\"Starting iteration {iteration} - Fine-tuning\")\n",
    "print(f\"Suffix: {suffix_iter}\")\n",
    "\n",
    "# Launch fine-tuning\n",
    "ft_resp = launch_finetuning_job(\n",
    "    output_filename,\n",
    "    base_model,\n",
    "    suffix_iter,\n",
    "    n_epochs,\n",
    "    n_checkpoints,\n",
    "    lora,\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuning job launched: {ft_resp.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_to_monitor_id = ft_resp.id\n",
    "new_fine_tuned_model_id = None\n",
    "\n",
    "print(\"Monitoring fine-tuning job...\")\n",
    "while True:\n",
    "    try:\n",
    "        status_response = together_client.fine_tuning.retrieve(job_to_monitor_id)\n",
    "        print(f\"Job {job_to_monitor_id}: Status = {status_response.status}\")\n",
    "        \n",
    "        if status_response.status == 'completed':\n",
    "            new_fine_tuned_model_id = ft_resp.output_name\n",
    "            print(f\"Fine-tuning completed! New model: {new_fine_tuned_model_id}\")\n",
    "            break\n",
    "        elif status_response.status in ['error', 'failed', 'cancelled']:\n",
    "            print(f\"Fine-tuning failed with status: {status_response.status}\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking status: {e}\")\n",
    "    \n",
    "    time.sleep(30)\n",
    "\n",
    "# Update current model\n",
    "if new_fine_tuned_model_id:\n",
    "    current_model = new_fine_tuned_model_id\n",
    "    print(f\"Updated current model to: {current_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = \"nikilrav/Mixtral-8x7B-Instruct-v0.1-self_taught_eval_ft_v3_sanity_iter4-e670c500-3ac461bb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating fourth iteration model...\")\n",
    "third_iter_model_results = evaluate_model_as_judge(\n",
    "    current_model,\n",
    "    together_client,\n",
    "    eval_dataset,\n",
    "    output_dir / f\"{suffix}_iter4_model_eval.jsonl\"\n",
    ")\n",
    "print(f\"Fourth iteration model results: {third_iter_model_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating base model...\")\n",
    "base_model_results = evaluate_model_as_judge(\n",
    "    base_model,\n",
    "    together_client,\n",
    "    eval_dataset,\n",
    "    output_dir / \"base_model_eval.jsonl\"\n",
    ")\n",
    "print(f\"Base model results: {base_model_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Base model:   {base_model}\")\n",
    "print(f\"Final model:  {current_model}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Base model accuracy:  {base_model_results['accuracy']:.2f}%\")\n",
    "print(f\"Final model accuracy: {third_iter_model_results['accuracy']:.2f}%\")\n",
    "print(f\"Improvement: {third_iter_model_results['accuracy'] - base_model_results['accuracy']:.2f}%\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save comparison\n",
    "comparison = {\n",
    "    \"base_model\": {\n",
    "        \"name\": base_model,\n",
    "        \"results\": base_model_results\n",
    "    },\n",
    "    \"final_model\": {\n",
    "        \"name\": current_model,\n",
    "        \"results\": third_iter_model_results\n",
    "    },\n",
    "    \"improvement\": third_iter_model_results['accuracy'] - base_model_results['accuracy']\n",
    "}\n",
    "\n",
    "with open(output_dir / \"evaluation_comparison.json\", 'w') as f:\n",
    "    json.dump(comparison, f, indent=2)\n",
    "\n",
    "print(f\"Saved comparison to {output_dir / 'evaluation_comparison.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-taught-evaluators",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
